{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference\n",
    "- [Boosting Wav2Vec2 with n-grams in Transformers](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Boosting_Wav2Vec2_with_n_grams_in_Transformers.ipynb)\n",
    "- [N Gram Language Model with KenLM + Tranformers](https://www.kaggle.com/code/umongsain/n-gram-language-model-with-kenlm-tranformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "# from datasets import load_metric\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data folder, CSV file names\n",
    "DATA_PATH = \"data/\"\n",
    "TRAIN_CSV = DATA_PATH + \"train.csv\"\n",
    "DEV_CSV = DATA_PATH + \"dev.csv\"\n",
    "TEST_CSV = DATA_PATH + \"test_release.csv\"\n",
    "\n",
    "TARGET_SAMPLE_RATE = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "# dev_df = pd.read_csv(DEV_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the transcripts from both CSV files\n",
    "transcripts = train_df[\"transcript\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the combined transcripts to a text file\n",
    "with open(\"text.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for sentence in transcripts:\n",
    "        file.write(sentence + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /m/home/home1/16/pix1/data/Desktop/geo_ASR_challenge_2024/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 50371 types 12508\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:150096 2:6665342976\n",
      "Statistics:\n",
      "1 12507 D1=0.697428 D2=1.12887 D3+=1.78536\n",
      "2 38509 D1=0.880434 D2=1.29385 D3+=1.35129\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing  994 assuming -p 1.5\n",
      "probing 1043 assuming -r models -p 1.5\n",
      "trie     504 without quantization\n",
      "trie     397 assuming -q 8 -b 8 quantization \n",
      "trie     504 assuming -a 22 array pointer compression\n",
      "trie     397 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:150084 2:616144\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:150084 2:616144\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:6673600 kB\tVmRSS:6288 kB\tRSSMax:1794956 kB\tuser:0.29004\tsys:1.44226\tCPU:1.73232\treal:2.43554\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 2 <\"text.txt\" > \"LM/2gram.arpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /m/home/home1/16/pix1/data/Desktop/geo_ASR_challenge_2024/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 50371 types 12508\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:150096 2:2318380288 3:4346962944\n",
      "Statistics:\n",
      "1 12507 D1=0.697428 D2=1.12887 D3+=1.78536\n",
      "2 38509 D1=0.88797 D2=1.31199 D3+=1.49201\n",
      "3 48214 D1=0.966904 D2=1.36581 D3+=1.19804\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 2067 assuming -p 1.5\n",
      "probing 2342 assuming -r models -p 1.5\n",
      "trie     995 without quantization\n",
      "trie     641 assuming -q 8 -b 8 quantization \n",
      "trie     954 assuming -a 22 array pointer compression\n",
      "trie     600 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:150084 2:616144 3:964280\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:150084 2:616144 3:964280\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:6657208 kB\tVmRSS:8128 kB\tRSSMax:1526596 kB\tuser:0.296676\tsys:1.04753\tCPU:1.34424\treal:2.00391\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 3 <\"text.txt\" > \"LM/3gram.arpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /m/home/home1/16/pix1/data/Desktop/geo_ASR_challenge_2024/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 50371 types 12508\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:150096 2:1134526464 3:2127237120 4:3403579392\n",
      "Substituting fallback discounts for order 3: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 12507 D1=0.697428 D2=1.12887 D3+=1.78536\n",
      "2 38509 D1=0.88797 D2=1.31199 D3+=1.49201\n",
      "3 48214 D1=0.970681 D2=1.39554 D3+=1.32098\n",
      "4 49884 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 3227 assuming -p 1.5\n",
      "probing 3784 assuming -r models -p 1.5\n",
      "trie    1551 without quantization\n",
      "trie     919 assuming -q 8 -b 8 quantization \n",
      "trie    1457 assuming -a 22 array pointer compression\n",
      "trie     825 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:150084 2:616144 3:964280 4:1197216\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:150084 2:616144 3:964280 4:1197216\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:6665140 kB\tVmRSS:6700 kB\tRSSMax:1325180 kB\tuser:0.336342\tsys:0.882898\tCPU:1.21927\treal:1.80091\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 4 --discount_fallback < text.txt > \"LM/4gram.arpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /m/home/home1/16/pix1/data/Desktop/geo_ASR_challenge_2024/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 50371 types 12508\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:150096 2:650277312 3:1219270144 4:1950832000 5:2844963584\n",
      "Substituting fallback discounts for order 4: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 12507 D1=0.697428 D2=1.12887 D3+=1.78536\n",
      "2 38509 D1=0.88797 D2=1.31199 D3+=1.49201\n",
      "3 48214 D1=0.970681 D2=1.39554 D3+=1.32098\n",
      "4 49884 D1=0.993918 D2=1.45073 D3+=1.15415\n",
      "5 50221 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 4402 assuming -p 1.5\n",
      "probing 5251 assuming -r models -p 1.5\n",
      "trie    2119 without quantization\n",
      "trie    1202 assuming -q 8 -b 8 quantization \n",
      "trie    1971 assuming -a 22 array pointer compression\n",
      "trie    1053 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:150084 2:616144 3:964280 4:1197216 5:1406188\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:150084 2:616144 3:964280 4:1197216 5:1406188\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:6681532 kB\tVmRSS:7052 kB\tRSSMax:1173076 kB\tuser:0.337338\tsys:0.803187\tCPU:1.14057\treal:1.83795\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 5 --discount_fallback < text.txt > \"LM/5gram.arpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LM/2gram.arpa\", \"r\") as read_file, open(\"LM/2gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LM/3gram.arpa\", \"r\") as read_file, open(\"LM/3gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LM/4gram.arpa\", \"r\") as read_file, open(\"LM/4gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LM/5gram.arpa\", \"r\") as read_file, open(\"LM/5gram_correct.arpa\", \"w\") as write_file:\n",
    "  has_added_eos = False\n",
    "  for line in read_file:\n",
    "    if not has_added_eos and \"ngram 1=\" in line:\n",
    "      count=line.strip().split(\"=\")[-1]\n",
    "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "    elif not has_added_eos and \"<s>\" in line:\n",
    "      write_file.write(line)\n",
    "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "      has_added_eos = True\n",
    "    else:\n",
    "      write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=12508\n",
      "ngram 2=38509\n",
      "\n",
      "\\1-grams:\n",
      "-4.611464\t<unk>\t0\n",
      "0\t<s>\t-0.055303354\n",
      "0\t</s>\t-0.055303354\n",
      "-3.623821\tdangon\t-0.124110006\n",
      "-2.6553433\tpro\t-0.33915424\n"
     ]
    }
   ],
   "source": [
    "!head -10 LM/2gram_correct.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASRw2v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
