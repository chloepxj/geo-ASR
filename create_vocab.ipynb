{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/16/pix1/unix/.conda/envs/ASRw2v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb#scrollTo=72737oog2F6U\n",
    "# Uses the GEO dataset\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"transcript\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary based on train and val set transcriptions\n",
    "def create_vocabulary(dataset):\n",
    "    vocabs = dataset.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=dataset.column_names[\"train\"])\n",
    "    vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"val\"][\"vocab\"][0]))\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "    vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "    del vocab_dict[\" \"]\n",
    "    vocab_dict[\"<unk>\"] = len(vocab_dict)\n",
    "    vocab_dict[\"<pad>\"] = len(vocab_dict)\n",
    "    with open('vocab.json', 'w', encoding=\"utf-8\") as vocab_file:\n",
    "        json.dump(vocab_dict, vocab_file, ensure_ascii=False)\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data folder, CSV file names\n",
    "DATA_PATH = \"data/\"\n",
    "# TRAIN_CSV = DATA_PATH + \"train.csv\"\n",
    "TRAIN_CSV = DATA_PATH + \"train_aug.csv\"\n",
    "\n",
    "DEV_CSV = DATA_PATH + \"dev.csv\"\n",
    "TEST_CSV = DATA_PATH + \"test_release.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12000/12000 [00:02<00:00, 4884.69 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4029.20 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 8112.84 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['file', 'transcript', 'audio'],\n",
      "        num_rows: 12000\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['file', 'transcript', 'audio'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['file', 'transcript', 'audio'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "\n",
      "Example files and transcripts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>augmented_audio/train_4549_aug.wav</td>\n",
       "      <td>la refiro doubs vormas in nordogcedinto di la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>geo/train_1390.wav</td>\n",
       "      <td>la rikeonon admenestras akintijo di protigto d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geo/train_5958.wav</td>\n",
       "      <td>teuj vortoj pofas surlogi life la survacon di ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geo/train_3524.wav</td>\n",
       "      <td>gei sen trofas teu lando me demandes al mi mim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>geo/train_3024.wav</td>\n",
       "      <td>la ankoroj ĉivi relates la gumuleĝon di la hes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>augmented_audio/train_502_aug.wav</td>\n",
       "      <td>ĉeu plumaroj istas semelaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>geo/train_1773.wav</td>\n",
       "      <td>la subaj partoj istas krezicaj sid la kapo kun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>geo/train_4502.wav</td>\n",
       "      <td>ĉekomponaĵoj ple malvrui apires in sireo da go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>geo/train_145.wav</td>\n",
       "      <td>teal la nomo honora unefirsetato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>augmented_audio/train_4022_aug.wav</td>\n",
       "      <td>ĝe istas la akregultura gaj gomirca cintro di ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 file  \\\n",
       "0  augmented_audio/train_4549_aug.wav   \n",
       "1                  geo/train_1390.wav   \n",
       "2                  geo/train_5958.wav   \n",
       "3                  geo/train_3524.wav   \n",
       "4                  geo/train_3024.wav   \n",
       "5   augmented_audio/train_502_aug.wav   \n",
       "6                  geo/train_1773.wav   \n",
       "7                  geo/train_4502.wav   \n",
       "8                   geo/train_145.wav   \n",
       "9  augmented_audio/train_4022_aug.wav   \n",
       "\n",
       "                                          transcript  \n",
       "0  la refiro doubs vormas in nordogcedinto di la ...  \n",
       "1  la rikeonon admenestras akintijo di protigto d...  \n",
       "2  teuj vortoj pofas surlogi life la survacon di ...  \n",
       "3    gei sen trofas teu lando me demandes al mi mim   \n",
       "4  la ankoroj ĉivi relates la gumuleĝon di la hes...  \n",
       "5                        ĉeu plumaroj istas semelaj   \n",
       "6  la subaj partoj istas krezicaj sid la kapo kun...  \n",
       "7  ĉekomponaĵoj ple malvrui apires in sireo da go...  \n",
       "8                  teal la nomo honora unefirsetato   \n",
       "9  ĝe istas la akregultura gaj gomirca cintro di ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = create_data_set(DATA_PATH, TRAIN_CSV, DEV_CSV, TEST_CSV)\n",
    "dataset = dataset.map(remove_special_characters)\n",
    "print(\"Dataset:\")\n",
    "print(dataset)\n",
    "print()\n",
    "print(\"Example files and transcripts\")\n",
    "show_random_elements(dataset[\"train\"].remove_columns([\"audio\"]))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12000/12000 [00:00<00:00, 125753.04 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 56436.50 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 69642.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ĝ': 0, 'a': 1, 'z': 2, 'ĉ': 3, 'm': 4, 'u': 5, 'ĵ': 6, 'p': 7, 's': 8, 'ĥ': 9, 't': 10, 'c': 11, 'f': 13, 'd': 14, 'l': 15, 'j': 16, 'b': 17, 'e': 18, 'h': 19, 'n': 20, 'ŭ': 21, 'r': 22, 'i': 23, 'v': 24, 'o': 25, 'ŝ': 26, 'g': 27, 'k': 28, '|': 12, '<unk>': 29, '<pad>': 30}\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "vocab_dict = create_vocabulary(dataset)\n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASRw2v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
